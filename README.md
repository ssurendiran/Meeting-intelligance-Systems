

```md
# ğŸ§  Meeting Intelligence Platform
### Scalable RAG-Based Transcript Understanding System

A production-oriented Retrieval-Augmented Generation (RAG) system for ingesting meeting transcripts, performing hybrid retrieval (dense + sparse), and generating citation-grounded answers with strict guardrails.

---

## ğŸš€ Overview

This system enables:

- Transcript ingestion
- Parsing and chunking with rich metadata
- Hybrid dense + sparse retrieval (RRF fusion)
- Citation-backed answer generation
- Multi-turn conversation support
- Clear roadmap for production scaling

---

## ğŸ— System Architecture

### Ingestion Flow

User Upload  
â†’ Validation  
â†’ Duplicate Check  
â†’ Parsing & Chunking  
â†’ Metadata Enrichment  
â†’ Embedding  
â†’ Qdrant Vector Store  

### Query Flow

User Query  
â†’ Rate Limit & Guardrails  
â†’ Memory Lookup  
â†’ Time/Speaker Parsing  
â†’ Query Rewriting  
â†’ Hybrid Retrieval (Dense + Sparse)  
â†’ Context Builder  
â†’ LLM Answer Generation  
â†’ Citation Guardrails  
â†’ Save Ask Memory  
â†’ Response  

---

## 1ï¸âƒ£ Synthetic Transcript Generation

### Purpose

Generate a synthetic meeting transcript based on user-provided topic and participants.

Used for:
- Testing ingestion pipeline
- Demo environments
- Simulating real meeting scenarios

### Production Alternative

In production:

- Real transcripts will be uploaded.
- Preprocessing occurs before ingestion.
- Synthetic generation can be completely removed.

**Impact:** This step can be eliminated.

---

## 2ï¸âƒ£ User Input Validation

### Required Inputs

- `topic` (string)
- `participants` (2â€“10 names, comma-separated)

### Validation Rules

| Condition | Result |
|------------|--------|
| Empty topic | HTTP 400 |
| Empty participants | HTTP 400 |
| < 2 participants | HTTP 400 |
| > 10 participants | HTTP 400 |

### Why Validation Exists

- Ensures meaningful transcript generation
- Prevents generic fallback meetings
- Improves LLM output quality
- Avoids system misuse

---

## 3ï¸âƒ£ Transcript Ingestion

### File Requirements

- Maximum size: **1 MB**
- Must contain at least one valid line in format:

[HH:MM:SS] Speaker: text

Invalid files â†’ HTTP 400.

---

### Duplicate Protection

- SHA-256 hash computed
- Compared against stored ingestion hashes
- If duplicate â†’ return existing `meeting_id`
- No re-embedding or re-indexing

Benefits:
- Idempotent uploads
- Avoids embedding cost duplication
- Prevents vector DB bloat

---

## 4ï¸âƒ£ Chunking Logic

### Strategy

- Tumbling window (no overlap)
- Default: 8 turns per chunk
- Final chunk may contain fewer turns

### Each Chunk Contains

- chunk_id
- Joined transcript text
- meeting_id
- file
- line_start / line_end
- time_start / time_end
- time_start_sec / time_end_sec
- speakers

### Metadata Enables

- Time filtering
- Speaker filtering
- Citation enforcement
- Meeting overview summaries

---

## 5ï¸âƒ£ Embedding Pipeline

### Batch Strategy

- 32 chunks per API call
- Reduces overhead
- Improves throughput

### Model

text-embedding-3-small  
1536-dimensional vectors  
Cosine similarity

### Retry Strategy

Attempt 1 â†’ immediate  
Attempt 2 â†’ 0.5s delay  
Attempt 3 â†’ 1s delay  
Attempt 4 â†’ 2s delay  
Fail after retries  

---

## 6ï¸âƒ£ Vector Storage (Qdrant)

### Collection

meeting_chunks

### Stored Per Chunk

- Dense vector (cosine similarity)
- Sparse vector (keyword scoring)
- Metadata payload

---

### Retrieval Strategy

1. Dense search
2. Sparse search
3. RRF fusion
4. Return top_k results (default = 10)

Mandatory filter:
- meeting_id

Optional filters:
- speaker_filter
- time_filter

---

## 7ï¸âƒ£ Answer Generation

### Flow

1. Build context (max 8 chunks)
2. Include metadata filters if applied
3. Send to LLM (gpt-4o-mini)
4. Parse structured JSON output
5. Apply citation guardrails
6. Return final response

---

## Citation Guardrails

- Citation must overlap retrieved chunks
- Clamp line ranges to valid ranges
- Drop invalid citations
- Dedupe duplicates
- If no valid citation â†’ return:
  "Not found in transcript."

---

## 8ï¸âƒ£ Ask Memory (Multi-turn Support)

Current:
- In-memory storage
- Lost on restart

Future:
- Redis / database-backed
- Shared across replicas
- Audit-ready

---

## 9ï¸âƒ£ Future Scalability

Planned improvements:

- Redis job queue
- Worker-based ingestion
- Async embedding
- Cross-encoder reranking
- Dynamic top_k
- Metadata-only citation
- PII redaction
- Semantic caching
- Langfuse tracing
- RAGAS evaluation
- Drift monitoring
- CI validation

---

## ğŸ§° Tech Stack

| Layer | Technology |
|--------|------------|
| API | FastAPI |
| Server | Uvicorn |
| LLM | OpenAI |
| Embeddings | text-embedding-3-small |
| Vector Store | Qdrant |
| UI | Streamlit |
| Validation | Pydantic |
| Config | python-dotenv |
| Package Manager | uv |
| Python | 3.12 |

---

## ğŸ“Œ Summary

This is a scalable, production-oriented Meeting Intelligence RAG system designed with:

- Hybrid retrieval
- Strict evidence enforcement
- Multi-turn conversation support
- Clear scaling roadmap
- Enterprise-ready extensibility
